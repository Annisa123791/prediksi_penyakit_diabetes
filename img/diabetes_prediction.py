# -*- coding: utf-8 -*-
"""Diabetes Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/119B2UKc_U7ODl32jGE-6RzzHM26PNevh

# PREDIKSI PENYAKIT DIABETES - KESEHATAN

##Business Understanding

###Membuat sebuah model machine learning untuk predictive analytics

#**Import Library**
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

"""#**Gathering Data**"""

df = pd.read_csv('diabetes.csv')
df.head()

"""#**Data Assesing dan Data Cleaning**

##Data Assessing (Penilaian Data)

Tahap ini bertujuan untuk mengeksplorasi struktur serta kualitas data yang tersedia. Fokus utama dari proses ini meliputi:

1. Memastikan ada atau tidaknya nilai yang hilang (missing values).

2. Mendeteksi keberadaan data yang terduplikasi.

3. Mengidentifikasi nilai-nilai yang tidak wajar atau outlier.

4. Mengevaluasi kelayakan data untuk dianalisis lebih lanjut atau menentukan apakah perlu dilakukan penyesuaian terlebih dahulu.

#**Data Cleaning (Pembersihan Data)**

Tahapan ini dilakukan untuk mengatasi berbagai permasalahan yang teridentifikasi selama proses penilaian data. Beberapa langkah yang dilakukan meliputi:

1. Melengkapi atau menghapus data yang kosong sesuai dengan konteks yang relevan.

2. Menghilangkan entri data yang duplikat agar hasil analisis tidak bias.

3. Menstandarkan format data, seperti penggunaan desimal, satuan ukuran, dan kapitalisasi, agar konsisten.

4. Menangani data dengan nilai ekstrem atau tidak masuk akal yang berpotensi mengganggu performa model.
"""

df.shape
df.info()

df.isna().sum()

df.duplicated().sum()

df.describe()

"""Hasil dari proses penilaian dan pembersihan data menunjukkan bahwa tidak terdapat nilai yang hilang (missing values) maupun data yang terduplikasi dalam dataset yang digunakan.

Selanjutnya, melalui fungsi info(), kita dapat mengenali fitur-fitur yang ada dalam dataset, yaitu: Pregnancies, Glucose, Blood Pressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age, dan Outcome.

Selain itu, dengan memanfaatkan fungsi describe(), kita memperoleh ringkasan statistik dari dataset, seperti nilai rata-rata (mean), standar deviasi, nilai minimum, maksimum, serta informasi statistik lainnya.

#**Exploratory Data Analysis (EDA)**

Eksplorasi Data atau Exploratory Data Analysis (EDA) merupakan tahap awal dalam menganalisis data yang bertujuan untuk memahami karakteristik data, mengidentifikasi pola, mendeteksi kejanggalan, serta mengevaluasi asumsi yang ada. Proses ini umumnya melibatkan teknik statistik dan visualisasi data.

Pada potongan kode di bawah ini, kita akan melihat perbandingan jumlah individu yang terdiagnosis diabetes dengan mereka yang tidak.
"""

diabetes = df[df['Outcome'] == 1]
non_diabetes = df[df['Outcome'] == 0]

num_diabetes = len(diabetes)
num_non_diabetes = len(non_diabetes)

labels = [f'Diabetes ({num_diabetes} orang)', f'Non-Diabetes ({num_non_diabetes} orang)']
sizes = [num_diabetes, num_non_diabetes]
colors = ['#F16767', '#FF9B17']
explode = (0.1, 0)

plt.figure(figsize=(8, 6))
plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140, wedgeprops={'edgecolor': 'black'})
plt.axis('equal')
plt.title('Perbandingan Diabetes dan Non-Diabetes')
plt.show()

"""#**Membuat histogram untuk setiap kolom numerik dalam DataFrame**"""

numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns

plt.figure(figsize=(15, 10))
for i, col in enumerate(numeric_columns, 1):
    plt.subplot(3, 3, i)
    df[col].hist(bins=20, color='#FF9B17', edgecolor='black')
    plt.title(f'Histogram: {col}')
    plt.xlabel(col)
    plt.ylabel('Frekuensi')

plt.tight_layout()
plt.show()

"""Kode di bawah digunakan untuk menghitung korelasi antar kolom numerik dalam DataFrame df, kemudian hasilnya diurutkan berdasarkan tingkat korelasi terhadap kolom 'Outcome'.

Nilai koefisien korelasi yang mendekati +1 menunjukkan adanya hubungan positif antar variabel, yang berarti ketika satu variabel meningkat, variabel lainnya cenderung ikut meningkat. Sebaliknya, koefisien yang mendekati -1 menunjukkan hubungan negatif, di mana peningkatan pada satu variabel diikuti oleh penurunan pada variabel lainnya.

Sementara itu, jika nilai korelasi mendekati 0, hal ini menandakan tidak adanya hubungan linier yang signifikan antara kedua variabel. Namun perlu dicatat bahwa ketidakhadiran korelasi linier tidak serta-merta berarti tidak ada hubungan sama sekali; bisa jadi hubungan tersebut bersifat non-linear atau lebih kompleks.
"""

sns.pairplot(df, hue="Outcome")

plt.figure(figsize=(12, 10))

correlation_matrix = df.corr().round(2)

sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.8, fmt='.2f',
            cbar_kws={'shrink': 0.8}, square=True, linecolor='white')

plt.title("Matriks Korelasi Antar Fitur Numerik", fontsize=20)

plt.show()

"""#**Dalam proyek ini, kita akan mendeteksi outlier menggunakan teknik visualisasi data boxplot. Kemudian, outlier ini akan ditangani menggunakan metode IQR (Inter Quartile Range).**"""

def box_plots_all_columns(df):
    num_cols = len(df.columns)
    num_rows = (num_cols + 3) // 4
    fig, axes = plt.subplots(num_rows, 4, figsize=(16, num_rows * 4))
    plt.suptitle("Box Plot before median imputation", fontsize=20)

    for i, column in enumerate(df.columns):
        row = i // 4
        col = i % 4
        sns.boxplot(df[column], ax=axes[row, col], color='#FF9B17')
        axes[row, col].set_title(f"Box Plot - {column}", fontsize=14)

    for i in range(num_cols, num_rows * 4):
        fig.delaxes(axes.flatten()[i])

    plt.tight_layout()
    plt.show()

box_plots_all_columns(df)

"""Berdasarkan visualisasi data yang ditampilkan sebelumnya, dapat diketahui bahwa terdapat sejumlah data outlier dalam dataset. Oleh karena itu, perlu dilakukan penanganan terhadap data-data yang menyimpang tersebut.

Interquartile Range (IQR) memberikan gambaran mengenai sebaran nilai dalam dataset. Nilai-nilai yang berada di luar rentang dari
𝑄
1
−
1.5
×
𝐼
𝑄
𝑅
Q1−1.5×IQR hingga
𝑄
3
+
1.5
×
𝐼
𝑄
𝑅
Q3+1.5×IQR dikategorikan sebagai outlier.

Q1 merupakan kuartil pertama, yaitu nilai pada persentil ke-25 dari data.

Q2 adalah kuartil kedua atau median, yaitu nilai tengah pada persentil ke-50.

Q3 menunjukkan kuartil ketiga, yaitu nilai pada persentil ke-75.

Nilai di bawah
𝑄
1
−
1.5
×
𝐼
𝑄
𝑅
Q1−1.5×IQR dianggap sebagai nilai terkecil yang mencurigakan, sedangkan nilai di atas
𝑄
3
+
1.5
×
𝐼
𝑄
𝑅
Q3+1.5×IQR dianggap sebagai nilai terbesar yang mencurigakan dalam dataset.
"""

Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)

IQR = Q3 - Q1

df_cleaned = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]

print(f"Dimensi dataframe setelah pembersihan outlier: {df_cleaned.shape}")

"""#**Menampilkan hasil visualisasi data setelah menerapkan teknik IQR**"""

def box_plots_all_columns(df):
    num_cols = len(df.columns)
    num_rows = (num_cols + 3) // 4
    fig, axes = plt.subplots(num_rows, 4, figsize=(16, num_rows * 4))
    plt.suptitle("Box Plot before median imputation", fontsize=20)

    for i, column in enumerate(df.columns):
        row = i // 4
        col = i % 4
        sns.boxplot(df[column], ax=axes[row, col], color='#FF9B17')
        axes[row, col].set_title(f"Box Plot - {column}", fontsize=14)

    for i in range(num_cols, num_rows * 4):
        fig.delaxes(axes.flatten()[i])

    plt.tight_layout()
    plt.show()

box_plots_all_columns(df)

"""Berdasarkan hasil perbandingan jumlah individu dengan dan tanpa diabetes, terlihat bahwa proporsi data penderita diabetes hanya sebesar 34,9%, sedangkan non-diabetes mencapai 65,1%. Hal ini menunjukkan bahwa dataset yang digunakan bersifat tidak seimbang (imbalanced). Ketidakseimbangan ini perlu ditangani karena dapat menyebabkan bias pada model serta menurunkan keakuratan hasil prediksi.

Untuk mengatasi ketidakseimbangan data, terdapat dua pendekatan umum yang bisa digunakan, yaitu oversampling dan undersampling, tergantung pada karakteristik dan kebutuhan dataset.

Oversampling dilakukan dengan menambah jumlah data dari kelas minoritas, cocok diterapkan pada dataset berukuran kecil. Teknik ini, seperti SMOTE, dapat meningkatkan performa model tanpa menghapus data yang ada. Namun, jika tidak digunakan dengan hati-hati, dapat menyebabkan overfitting.

Undersampling melibatkan pengurangan jumlah data dari kelas mayoritas, biasanya digunakan pada dataset besar. Metode ini dapat mempercepat proses pelatihan dan menyeimbangkan distribusi kelas, tetapi juga berisiko menghilangkan informasi penting jika data dikurangi secara acak.

Dalam kasus ini, karena ukuran dataset tergolong kecil, pendekatan yang dipilih adalah oversampling, dengan tujuan untuk menyeimbangkan jumlah data antar kelas dan meningkatkan kinerja model secara keseluruhan.
"""

X = df.drop('Outcome', axis=1)
y = df['Outcome']

def apply_smote(X, y):
    smote = SMOTE(random_state=42)
    X_resampled, y_resampled = smote.fit_resample(X, y)
    return X_resampled, y_resampled

X_resampled, y_resampled = apply_smote(X, y)

class_distribution = Counter(y_resampled)

print("Distribusi kelas setelah oversampling:")
for label, count in class_distribution.items():
    print(f'Kelas {label}: {count} sampel')

counter_before = Counter(y)

oversample = SMOTE(random_state=42)
X_over, y_over = oversample.fit_resample(X, y)
counter_over = Counter(y_over)

plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
colors = ['#FF9B17', '#F16767']
plt.bar(counter_before.keys(), counter_before.values(), color=colors)
plt.xticks([0, 1])
plt.xlabel('Kelas')
plt.ylabel('Jumlah')
plt.title('Sebelum SMOTE')
plt.ylim([0, max(counter_before.values()) + 100])

for i, v in enumerate(counter_before.values()):
    plt.text(i, v + 5, str(v), ha='center', va='bottom')

plt.subplot(1, 2, 2)
plt.bar(counter_over.keys(), counter_over.values(), color=colors)
plt.xticks([0, 1])
plt.xlabel('Kelas')
plt.ylabel('Jumlah')
plt.title('Setelah SMOTE')
plt.ylim([0, max(counter_over.values()) + 100])

for i, v in enumerate(counter_over.values()):
    plt.text(i, v + 5, str(v), ha='center', va='bottom')

plt.tight_layout()
plt.show()

"""Data yang tidak seimbang (imbalanced data) telah berhasil ditangani, data sekarang sudah seimbang dan siap digunakan untuk tahap pengembangan model.

#**Data Splitting**

Membagi dataset menjadi dua bagian, yaitu data pelatihan (train) dan data pengujian (test). Data pelatihan digunakan untuk melatih model, sedangkan data pengujian berfungsi untuk mengevaluasi kemampuan model dalam menggeneralisasi pada data baru yang belum pernah dilihat sebelumnya.

Setelah melakukan beberapa percobaan (sebelumnya menggunakan pembagian 80:20), pada kasus ini dataset akan dibagi dengan rasio 70:30, yakni 70% untuk data pelatihan dan 30% untuk data pengujian.
"""

X = df.drop('Outcome', axis=1)
y = df['Outcome']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print("Data Latih (Train):", X_train.shape)
print("Data Uji (Test):", X_test.shape)

"""#**Standardisasi**

Sebelum membangun model machine learning, dilakukan proses standardisasi terlebih dahulu. Tahap ini bertujuan untuk mengubah fitur numerik dalam data sehingga memiliki nilai rata-rata nol dan standar deviasi satu. Standardisasi penting karena banyak algoritma machine learning akan berfungsi lebih optimal dan lebih stabil jika fitur numerik berada dalam skala yang seragam. Dengan demikian, fitur-fitur tersebut diperlakukan secara konsisten, yang dapat membantu meningkatkan performa model.
"""

num_pipeline = Pipeline([('std_scaler', StandardScaler())])

X_train_prepared = num_pipeline.fit_transform(X_train)

X_test_prepared = num_pipeline.transform(X_test)

"""# **Modelling dan Evaluate**"""

def evaluate(clf, X_train, y_train, X_test, y_test, train=False):

    classes = y_train.unique() if train else y_test.unique()

    data_type = "Train" if train else "Test"

    pred_func = clf.predict(X_train) if train else clf.predict(X_test)

    clf_report = classification_report(y_train, pred_func) if train else classification_report(y_test, pred_func)
    accuracy = accuracy_score(y_train, pred_func) * 100 if train else accuracy_score(y_test, pred_func) * 100

    cm = confusion_matrix(y_train, pred_func) if train else confusion_matrix(y_test, pred_func)

    plt.figure(figsize=(4, 2))
    sns.heatmap(
        cm,
        annot=True,
        cmap='Blues',
        fmt='g',
        xticklabels=[f'Predicted {class_name}' for class_name in classes],
        yticklabels=[f'Actual {class_name}' for class_name in classes]
    )

    print(f"{data_type} Accuracy Score: {accuracy:.2f}%")
    print(f"CLASSIFICATION REPORT:\n{clf_report}")
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'Confusion Matrix ({data_type})')
    plt.show()

random_forest_model = RandomForestClassifier(
    n_estimators=50,
    max_depth=5,
    max_features='sqrt',
    random_state=42
)

random_forest_model.fit(X_train, y_train)

evaluate(random_forest_model, X_train, y_train, X_test, y_test, train=True)

evaluate(random_forest_model, X_train, y_train, X_test, y_test, train=False)

"""#**Hyperparameter Tuning**"""

base_model = RandomForestClassifier(random_state=123)

param_grid = {
    'n_estimators': [50, 100, 200, 300],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth': [5, 6, 7, 8],
    'criterion': ['gini', 'entropy']
}

grid_search = GridSearchCV(
    estimator=base_model,
    param_grid=param_grid,
    cv=5,
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train, y_train)

print("Best Parameters:", grid_search.best_params_)

best_rf_model = RandomForestClassifier(
    random_state=42,
    max_depth=grid_search.best_params_['max_depth'],
    n_estimators=grid_search.best_params_['n_estimators'],
    max_features=grid_search.best_params_['max_features'],
    criterion=grid_search.best_params_['criterion'],
    n_jobs=-1
)

best_rf_model.fit(X_train, y_train)

evaluate(best_rf_model, X_train, y_train, X_test, y_test, train=True)

evaluate(best_rf_model, X_train, y_train, X_test, y_test, train=False)

"""#**BEST MODEL**

Berdasarkan hasil evaluasi model, yaitu :
- Train Accuracy Score sebesar 98.70%
- Test Accuracy Score sebesar 75.32%
"""